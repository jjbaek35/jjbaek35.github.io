` makes the tax successful that opens the Defenders up to be being able to mitigate that attack and last but not least uh problem with a data problem which is if we want to look at a bunch of attacks and actually learn the information it'sreally hard to collect that uh it's expensive right even if you hire many red teams and you have them attack your network and you collect all that data you create a data set well what happens if you want to do it for a second Network or you want to change the first Network slightly and see if it made it better or worse are you going to bring all those people back probably not so the question is can we use simulative tax provide some kind of useful information for us teams okay so to give some literature contexthere modeling the text is not necessarily new that's that's going back a long way but the early research in that area has really focused on this concept of just description driven model so this is these are the types of attacks that we expect to see these are the types of nodes in the network these are the types of behaviors a fast attacker a slow attacker it's very description driven and if you craft your descriptions wrong or if you find an adversary that doesn't fit into those boxes that's not going to work very wellmore recent work has really focused on I Fidelity simulation so we have uh devices we have versions of operating systems patch versions uh what vulnerabilities are existing in the software that's installed on these and this gives you quite quite good data but it's also challenging to collect in the sense that if you have a network that's deployed in practice or that hasn't even been deployed yet you might not have that information or that collecting that information might be very hard so thereare there are pros and cons to these different approaches that hopefully be impact in terms of measuring security there are more metrics to measure security than you can count this is a particularly good survey that I liked but a lot of these metrics are really based on information that you might not have like what is the likelihood of a vulnerability appearing in my network how do you how are you supposed to get that number don't know um how much is it going to cost if an attacker compromises my network againyou might not know that um so there are still I think gaps in measurement that we can hopefully Rectify by giving very specific and actionable advice with our metrics in terms of recommending changes to networks to make them better this area is a little bit less explored um but there has been some work here so uh early research has kind of focused on okay if we look at an attack while it's happening can we sort of learn the behavior of the attacker and change the behavior of the network in real time as the attacker moves throughthe challenge with that is that detecting limited attackers in the network is easier said than done intrusion detection is its own field of research and network activity in general has a very long tail which means it's hard to identify when attacks are happening and to be able to pull out that adversarial Behavior uh more recent work has sort of gone back to the other end of the pendulum where okay you know what let's just focus on General Frameworks you know you should have your network be like this itshould follow these high level rules um which is great those are very useful but they're not very specific they're not very action so a Defender who doesn't have especially who doesn't have domain expertise is not going to know what to do without necessarily and the last point is this idea of counter adversarial strategies and this has been barely changed um the U.S cyber command is starting to look at this so they have this idea of yes we want to impose costs as a strategic choice but it leaves how to actually do that sort of up to the reader um more recent work has done a deeper conceptual exploration of this idea of sludge so sludge here they use to describe uh slowing down the network when the attacker is moving through it and not when the attacker is not there but they really rely on okay well this is a Honeypot Network so anyone here must necessarily be an adversary so we can slow that down without muchbut again it's height it's mostly conception okay so now we'll talk about what I've done uh but we need to set up a few preliminaries first uh in our Network model we intentionally have kept everything abstract and that's for a couple reasons so one it allows us to make it very easy for the user to collect information from their Network they don't need to know about versions they don't need to vote no about operating systems they don't need to know about what vulnerabilities arethere they just need to know about what nodes are in the network how they connect so the nodes in the network uh can have a source some set of sources which could be up to the entire set of the network and Source nodes are where there's a possibility that the adversary could get it so that could be through a phishing attack that could be through some web server vulnerability uh it doesn't matter we leave that abstract there's also this set of goals goal nodes which are nodes where you don't really want the adversary to get toum it might be sensitive data maybe you just don't want the adversary to be able to move through your network very much at all and so you just make everything a goal node with an employee and you can assign optional ways um in the edges we have logical links so we sort of leave the problem of securing physical links like via an sdn to a to other work but logical links where nodes can communicate with each other and interact and Trust relationships which are directed and so typically you would think of a trust relationship as like anSSH key you have a machine that holds an SSH key that authorizes it for administrative access to another machine but because we've intentionally kept that abstract we can actually use this model to represent any kind of network so we could use this for an organ an org chart and the trust relationship would be oh well the supervisor can tell the uh you know their their subordinates what to do um or a social media graph where some information is being passed around and you know this is under this informationcomes from a particularly trustworthy source so I'm going to accept it we can represent most kinds of networks that can follow this uh this paradigm in our attack models is like what the attacker can do uh in the network and we try to keep the attacker as powerful as we can so following our permeability assumption exploits always succeed so when the attacker decides to exploit a system they always are able to do so but exploits do carry a cost so we limit the maximum number of exploits the adversary can do to some value in helland if they're able to capture this root of trust they can exploit that for free right they're just replicating Behavior that's already in the network so it's very easy for them to do that uh we also assume that attackers know the details of our defensive approach so we can't do anything here that's going to be predictable for predictable then the adversary will just change their behavior uh to avoid uh they may or may not know the layout but now our tool supports both um and our adversaries as they're movingthrough the network can follow any behavioral policy so that means they might be low and slow they might be like doing a smash and grab uh they could just be moving randomly through the network which we found was actually one of the hardest to mitigate um because you can't really predict what they're going to do um but again all of this is happening at the planning stages so we've let real-time behavioral changes to the network as sort of outside of the scope of this one the last idea that I need to introducehere is this idea of Frontier expansion so uh we have our sources and our goal nodes and the attacker can start from a source node and sort of spread their influence through the network and as they capture nodes they threaten additional nodes but it's not always clear where they're coming from because they might have multiple points of Entry to a particular number so for example here uh in our graph f is being threatened so if the next node that was captured was F we can't say for sure if that attack would have come from C orwould have come from d so this this idea of Frontier expansion allows the attacker quite a lot of flexibility in how they move through the network okay so with those preliminaries out of the way we can take a look at our first part our first challenge which is measure if we want to use graph theoretic measures uh those are great because they're relatively cheap um and they're nice and objective um but they also tend to be relatively predictable and often deterministic which is going to be a problem if theattacker can sort of predict what we're doing they know the details of our approach so maybe pure graph Geographic measures might not work um domain tailored measures so these are sort of the security measures that I mentioned before um they might be better but they require expertise to evaluate um and they don't necessarily translate well right if you have a metric that measures the security of a scada powered power grid if you want to take that measure and you want to apply that to a uh an ad hoc mobile network it might not applyso the question is can we sort of mix these okay what we'd like to be able to do is come up with some kind of note impact so the node impact score should measure you know how important is this node uh in the network to the attackers what role does it play how important is it um so for example highly Central nodes which provide great Vantage points for the attacker they should have a higher score a higher impact score um but also nodes that are near to the goal or our goals should also have a higher impact scorebecause the attacker is actually making progress towards their success so how can we how can we balance things so to do that we borrowed an idea from uh a a different graph theoretical measure personalized page ranking and personalized pagerank is used uh was originally developed as a mechanism for ranking websites so you started at some particular website and you click a link on that website to go to another web page and you click a link on that website to go to another web page and you do a random walk through the networkby clicking links and then at some point you get bored and you click a bookmark and that bookmark teleports you somewhere else in the network and that teleportation is called the seed Factor the C Factor controls where you teleport to well if we take our source and our goal sets and we use them as our seed that allows us to balance exploration of the network for those that are near to sources and exploitation of the network for nodes that are near to the goals so we get nice balance there but the question is what do we do with ourtrust subjects if we invert our trust edges so instead of pointing from the node that has the that holds the trust to the node that it controls we reverse it that allows us to back propagate the influence of that trust through the network so the flow from this node now is going up here and then back over to here um and because this seed Vector changes we exclude captured nodes from the thing as we're going along for each step I uh that allows us to dynamically change these over time and Computing PPR is relatively inexpensive so that's a costthat is that we're willing to pay so what does that give us all that gives us let's say um okay so that allows us to take our node impact score based on the equation that we just saw and use that to compute the overall impact of an attack the reason why we need that is because the node impact score is hyper local it's only applying to a single node at a single time step in a single attack which doesn't necessarily tell us what everything that we want to know but if we combine it stepwise over the courseof the attack that allows us to give an approximate evaluation of the of the attack spell what does that give us well if we use this attack impact that allows us to measure a few different things success ratio so how often does an attacker succeed across some subset of tax uh when they do succeed what are the balance of those attacks right what's the what are the what's the mean number of attacks that they use the median the standard deviation the max Min which tells you things like you know the depthof your defenses or how much how difficult your network is to attack we can also do that with the Trust balance as well how often is the adversary using trust to violate the network versus using attacks which carry a much higher cost to the attacker and so from these metrics we can do a b comparisons so these two networks are almost identical there's only one Edge difference and that's the Edge from I to H so we've just broken this Edge perhaps this would be like an administrative system that was holding a credentialthat they had forgotten about and they just deleted it so in the graph on the left uh the attacker succeeded about 55 of the time which is not great right you don't want the attacker to succeed 55 of the time um it was not terrible because they typically had to use most of their attacks they used four of their five attacks on average so that's not too bad we had decent uh constraints there but the trust usage is higher than we would like to see right that's that's a that's a fair amount of trust so the questionis how much did we improve by just removing that one Edge about two-thirds so the attacker succeeds only about a third as often as they did under the other graph just with the removal of that single bench it also costs them more to do so so their success is reduced and their costs are higher that's our counter adversarial thing coming back they also were able to make use of trust edges much less often yeah go ahead and K the source of destination uh A and K are two potential Source nodes and H E and G are Golden'sin this case uh this is a this is an accumulation over several attacks um so this this doesn't visually represent any one specific attack in that case I'll I'll point it out there is one later okay so we have these measures and they're useful but what we really would like to do is to give actionable advice how can we do that well our users most users most Network operators don't have any experience with network security it's people running networks in their own homes in their small businesses theydon't have any expertise with that and so what we'd like to be able to tell them is like this is a very simple step that you can take that will make your network quantitatively more secure well if we could rank those substructures that helps us because again our representation is very abstract we don't know anything about the functional requirements of this network maybe node a and node B have to be connected well if we could rank them then if the top ranked one is a you should disconnect A and B but the defenderknows that's not possible they can just go to the next one they can consider their own functional requirements without having to share them with us so to do that to get this ranking we can borrow an idea we think we can borrow an idea from information retrieval to synthesize some some some score for a feature f and relevant features are going to have a pop they're going to be pulled from positive class much more often than the negative class the irrelevant features are the reverse and to sort of give a give an example uhfrom the information retrieval space let's say you're looking for documents that were related to the code your features are the words in the documents if you see in the document the word blasnost that's a relevant feature most likely that document is going to be about the Cold War but if you see Henry VII well probably it's not going to be about the Cold War right it's giving you a negative signal but if you see the word however you can't really say right it's it's non-relevant it doesn't push you one wayor the other so how how can we how can we use this right we're still missing a few things we're missing our features and we're missing our relevance classes so first let's see how we can get these features here's the here's here's an actual attack a concrete attack uh We've observed this sequence of nodes uh being captured but this doesn't tell us very much by itself because of that Frontier expansion from before what we actually need to do is we need to project this attack into a featurespace this feature space so we can project into a higher Dimension um where for example when we capture an age we could have captured H coming from I H coming from f but we don't know if they collect all of those features and most importantly we preserve this causally plausible idea so in order to reach H you had to come from one of those three not necessarily from I which happens to immediately perceive each so that gives us our attack features but we still need to come up with classes well now our attack impact is showingback up again because it turns out that when you look at a bunch of attacks and you plot the attack impact there's a characteristic distribution for graphs that has a nice well-defined elbow where you have some attacks here that have a high impact that happens to find some flaw in the network and you have a bunch of attacks here that didn't find that flaw that should sort of meandered and tried to explore the network and find uh some other information well if we just drop a line right there that gives us our classification our initialclassification we have our positive class here the attack weren't very impactful but we still have a problem just start class is super imbalanced what do we do about that if we try to use the imbalance classes directly it's not going to give us good conclusions so we need to try to rebalance that and to rebalance that we we take two ideas from the literature the first is this idea of atonement until my points look at nearest Neighbors so in this among these features we look at the at all the features at theirPairs and we look at the cases where the nearest neighbor to a majority class was in the minority class and that's atonement when they cross classes and the purpose of atomically is to find near the decision backward with the assumption that near the decision battery there's going to be some noise and to remove that noise and to mitigate it we just remove all of the features that are in the majority class so now we're making progress getting closer we have a nice clean decision boundary between ourgroups um but we still have quite a lot of red nodes compared to our Blue Notes here so borrow another idea Paul minority over sampling and we look at the feature space that we're describing in our minority class we interpolate new features within that space uh into our positive class and now we're getting a lot closer here to balance so we have our features our attack features we have our balanced classes so let's go back and compute our feature relevance you plug everything in we plug our features and our relevant classes andour irrelevant classes in and that gives us a score if the score is positive so much much greater than zero then that attack is very useful to attackers and those are the ones that we're going to want to mitigate if they're near zero doesn't really tell us much we don't really know what to do with those we could just kind of ignore those but if it's much less than zero it's actively counter adversarial if the attack if you see an attacker see that feature that attacker is not in for a good timeso you want to preserve those or potentially add new ones if you can also because every feature that we observe has a score we can rank it we can order it and we can present that to the user for them to decide what to do again we preserve our functional constraints because they just need to go down the list until they find an acceptable mitigation that they can do change the network they can rerun the experiments and see if they can do any in ad comparison to see what effect it happened we've sort of been glossing over theelephant in the room here so far which is we have all these attacks that we're looking at to compute our attack information to our attack impact support where are we getting these attacks from civility now we're going to talk about how because we this problem itself is also quite difficult we can't just look at the entire feature space of every possible attack that can happen on the network it's way too big especially for complex Networks um and our users who don't have domain expertise don't they don't know theycan't answer any of this they don't know if there's if there is a flaw in the network how many flaws there are the density of the flaws how easy those flaws are to discover where those flaws might be they don't know any of that so asking them to come up with some kind of appropriate simulation budget is not realistic we need to not only simulate the data but we need to simulate the data for them with minimal intervention so how can we do that we can ask them to just provide two pieces of informationthe first is Target coverage so of the space of all possible attacks how much of that space do you want to explore so if all possible attacks is 100 you want to explore 99 of that space okay that's the first thing like people can people can kind of understand that uh and the other is this idea of a threshold so how sure are it do you want to be that you're still making progress you might just be turning the crank and not actually making any progress in your simulations and you should just stop so this is sort of you can think of this assort of sensitivity to cost well if we want to compare things to the space of all possible features we need to be able to approximate the space of all possible features we just said it it's not realistic to exhaustively compute that so how can we process we can't do it using the stateful evolution with the frontier expansion it's too expensive because we need to keep track of things like what nodes uh have we seen where can we go next which nodes are threatened and moving through the network it's just very expensive interms of memory and in terms of time it's difficult to parallelize that but if we rely on random walks which will which use very very little statement that can be a useful building block for approximating this space in a much more efficient way so there there are two things that we need to go over so the first way that we can do that is by using what I call overlocking but it's basically just traditional random walk so we start at the source nodes and we just run many many random walks uh from those sources and we don'tconstrain them to the attack budget we led to be very very long and what this gives us is a set of features of OverWatch features that are causally consistent for our precedence strengths so in order to see node C I had to see node B first they follow those precedence constraints but there's a problem because they're ignoring our budget constraints they violate this budgetary causality so how do we how do we solve that problem well we introduce a different way of doing Concentra of doing brand walks which isthis idea of concentric residual walks so if we assign every node in the network a residual budget which is based on its minimum distance from a sort from any Source node then and then run random walks of that residual for every note of the Network that has a non-negative residual that gives us another set of features are concentric residual walk features there they are causally consistent under our budgetary constraints because they can only spend up to their residual budget so here they're only of lengthtoo but they're inconsistent under precedence causality because you might end up under node is that you know a dead end in the network but still has a residual budget you know in practice the attacker would have to expand to that node and then come back but it's not possible to observe a feature going back the other way or it shouldn't be well it's very convenient then that these two methods of random walking are mirror images of each other one is because each one is consistent under one and inconsistent over theother if we intersect them it prunes all the causality violations that we have and now we have a very efficient way of approximating our feature space without using state so now we need to turn our attention to the other thing we have our possible feature space we need to now look at the threshold when you first start looking for features it's very easy for you to discover new features because you haven't seen anything yet everything you see is new your probability of seeing something new is one but as you go along as you simulate moreand more often it becomes harder and harder and harder for you to actually collect that information mostly you're seeing things that you see that you've seen before and this curve is actually pretty well fitted by a double exponential decay which means that it's extremely expensive to discover that long term and we need to compute the likelihood of observing some new feature right because we want to decide if we're going to continue running simulations or if we're going to stop so how do we decide that well if we lookat the entire curve we're going to really overestimate How likely it is that we're going to see a new feature because at the beginning our probability was one so we don't want to look at the entire curve but if we have this fixed trailing window size that's also a problem because not every network is as easy to explore as others we need a dynamic window size so how can we solve that well we look back again that looks like a pretty nice curve right there you can just draw the line right at the elbowif you draw the line at the elbow and we call that J then now we can compute our trailing Discovery efficiency just for that dynamically sized windows and that gives us a value it doesn't just give us about it right we can compare that value directly against the Theta to determine our termination condition but it actually gives us something else as well which is how large each phase of the simulation should be because we don't want to just have to decide necessarily at every single step okay I ran one simulation soI want to run another simulation do I want to draw another simulation we could just solve for w here we solve for w and that allows us to approximate the size of the upcoming phase so how many simulations should I need to run in order to get the information that I need to reach my threshold and after each phase we can take the additional information that we collected recompute our curve recompute all our values and refine our solution as we're going along so four different topologies you don't have a fixed number of phases everytopology has different number faces each phase is a different size all we care about is did we get to the goal that we were looking for did we reach our Theta did we reach our absolute initially uh you pick uh a small initial budget and I'm going to show the results that how you pick W doesn't really matter so let's look at uh let's look at some data so we're going to look at a couple different data sets so first we're going to look at some synthetic data sets that are generated for random graphs across fivedifferent families of random crops and this is kind of to illustrate that different kinds of graphs have different behaviors in terms of their resilience to attack let's say and we also need to see if that translates if that behavior translates to real world topologies so we picked five real world topologies to look at uh Bell Canada BTN and Titan are National or International uh CDN Networks and Cessna and gar are National research networks uh for chakia and Italy respectively can you tell me what the difference isbetween a power lot and a scale free Network so the scale-free networks are quite spindly uh in the sense that um they are connected with they're they're quite clustered let's put it that way um the power line network uh is more well connected I would say um than the than the scale three networks are um so we also need to compare uh the the ranking results across a couple different solutions now there's not really any competing uh solution that does exactly what we're trying to do but we can compare against some differentheuristics and methodologies that we think might make sense so we might look directly at using node impact uh without going through this additional uh recognition step we might just use an edge between the centrality directly you know how well can we do by just using uh graph theoretic measures without going through this will work um and then we also look at R2 uh R2 approaches so this is the sort of proposed feature relevance that I've described um but we also look at the inverse so what happens if we have this ranking ifwe just flip it so all of our positive numbers become negative well we should expect to see that the attacker succeeds more often and at lower cost so we're going to see if that actually happens here we're mainly looking at two grams and that's just because for many of these it's not well defined for features of other sizes but again uh this approach could apply to features of arbitrarily long size assuming you have the computational resources to do that so we'll start with the uh with thesynthetic graph and um we can see that actually there is quite a big difference between the different families so for example um using node impact by itself uh does a pretty good job for the erdogs Randy traps um almost as well as our approach but under every circumstance our approach is able to outperform um and substantially outperform this dark blue line which is the original unmodified Network so we're able to substantially reduce the attacker's success um sort of across the point but different approaches uh or differentfamilies have different characteristic behaviors so it's much easier for example to mitigate in a skill free case because they're it's just easier to uh guide the attacker away from areas where you don't want them to go we also need to look at the other side of the coin which is our cost so here we want cost to be higher right so we want this success to go down and the costs are to go up and we do see that cost goes up um again in some cases it uh we're quite near to just using node impact directlybut in some cases we're quite different so for scale free graphs um the uh are our approach is really increasing the cost relative to the um the the vanilla note impact um and if we look at our inverse we do see that in general the cost is either near the bottom or we can see that for a success as well so uh the inverse of our approach is near the top it doesn't see as much of a benefit as we as we see it with our defensive approach and that's just because um there are not a lot of uh features that aregoing that are misleading there are not a lot of misleading features in the network uh for us to remove to help the attacker so our hands are kind of tied in terms of how much better we can get before that when we apply that to the real world we sort of see in a similar pattern which is that it's quite different depending on the graph um in Bell Canada it's all it's relatively tightly clustered but when it comes to uh to cessnet we see a huge benefit in our approach and node impact isn't able to quite keep up uh as muchas as our underlying approach but again here our inverse is really not as is not helping us as much as we would hope it to and that's because there's really just uh these are practical networks that were just designed to be as efficient as possible they're not redundant again go ahead can you just remind me how many nodes roughly or yes so uh these networks are roughly around 50 to 60 nodes each with a couple hundred edges so we see in some cases like in uh like in gar we're getting pretty close tousing vanilla node impact um but if we look at the costs we see a big difference so using node impact by itself does do a fairly decent job in many cases of reducing adversarial success but it does that in a very interesting way which it typically does by just partitioning trying to partition the adversary away from the rest of the network which is not what we want to do because that reduces the adversary's costs they just hit some data and they say um that's it I'm done what we'd like to do is keep theirsuccess low which we are able to do competitively but we also want to maximize their costs so the teal approach here which is the proposed approach is able to do both keep the success low and keep costs High okay so now we're gonna we're gonna switch over to the simulation so here Boris this is to your question of how do we make this initial clutch so if we picked an initial budget of 20 versus initial budget of 200 does it actually make a difference and we see that the pattern is almost identical um which means that our approach ofrevising and revising and revising actually does pretty well even when we give it only a little bit of information at the beginning so even 20 simulations is enough to come up with a pretty good uh estimation of the overall cost that we want the Ensemble to to consume if we look at Target coverage we see more interesting patterns so here if we set Target coverage to 90 well for cessnet and gar you can do that in your initial budget you don't even need to do any additional phases which is interesting that meansthat the features in cessnet and gar are very easy to discover in general getting to 90 takes not a lot of effort but for Bell Canada you need to actually put in a fair amount of effort to get there for tainet it really matters it depends on the permutation that you're looking at when you set it to 99 that start that starts to flatten out and you do see that it is in general easier to discover features in cessnet gar um but now taina is dominated what that means is that the discoverability of features in Tainanhas a very long tail there are a lot of features in Thailand that are hard to discover you need to work hard to get this so that's our Epsilon what about our Theta well Theta again this is sort of analogous to our cost sensitivity um has a big difference in total final costs so if we want to be 99.999 sure that we're making progress before we give up well we're going to give up pretty easy um in general we're going to give up after our initial budget with a few exceptions here uh except for Thai net so tainet we need to do more work we think that we we can see that long tail ahead of us and we want to continue to explore that if we set our Target partial to 99 well now all of a sudden our maximum cost has more than double um so this threshold can be relatively uh important in setting the overall cost ofthe simulation Ensemble um but uh should still be relatively understandable for users to choose so to just kind of quickly wrap things up and package things together um we have this rise of increasingly complex systems devices that are talking to each other like crazy we looked at different strategies on how we might mitigate those so if how what we can do by partitioning information among nodes and what we can do by actually assessing the resilience of the networks themselves identifying it and surfacing that to usersof course there are many many open challenges that remain so one thing is we've just the the topology that we've described you need to know what the topology is it's easier than getting some information but it's not necessarily easy especially if you're dealing with a very large system or an undocumented system or a legacy system you might not even know what devices are there and what's communicating with one so if you have the ability to automatically discover that and build this networkwithout user intervention that would make it much more accessible and much more practical uh in terms of scalability we've in this in this work we've pretty much looked at like 100 scale crafts but if we could expand this if we could scale this up to million or billion scale graphs well now the applications could be much much greater potentially at the level of of a Nation or global we also haven't really looked at uh the heterogene heterogeneity of the network so we have really made the assumptionsthat one node equals one attack but in practice that's not really realistic if you get if you find a zero day that works against a Windows machine well it works against all the windows machines in the network so one potential way that you could put one potential way that you could extend this is by instead of using this Multiplex graph of trust and connectivity you could expand that to a hyper graph of node equivalence classes that would allow you to sort of address this challenge and then the last sort ofsimplification that I think could could uh expose new work is this concept of adversarial Behavior so we've left this idea of adversarial Behavior sort of untouched the simulator is randomly generating these attacks but if you gave it a strategy an adversarial strategy an adversarial generator then you'd be able to generate data that more closely aligns with what you know about your adversary if they have certain characteristic behaviors and that would allow you to really tailor your mitigations to a particular adversaryyou could also discover for example uh behaviors that are very effective against a particular Network you might find oh you know this is a network that I'm looking at uh and it turns out that low and slow attacks while typically are very effective just don't work that well against this network for some reason based on the data but so these are things that could that could use some additional work I think so just to quickly uh conclude yeah go ahead for us okay so for the last one we do have some heuristicthat you use for the attack that you're simulating uh it's random by default if you don't provide a uh if you don't personal student completely random there's no precedence of using credentials versus attacks there is a there is a there is a preference uh for using credentials over attacks so there's a cost reference the adversary given the choice we'll use credentials over uh attacks but uh if their only choice is to use an attack then they will choose the location of that attackat random Okay so this permeability assumption that we've been talking about during these shows hopefully I've been able to show that distribution and decentralization are not antithetical to security we can secure these systems there are realistic affected and efficient methods that can be ready and used today and these complex systems that that we're seeing the rise of and as sort of become more more common um can be secured as long as we change our perspective from this concept of perimeter to this concept ofpermeability encounter adversarial thank you any patience from food Turkish and complex systems and the first motivation was the use of connected devices growing them some of those will be in and out and also being if you can think of some applicable domain it says autonomous vehicles are connected to your internet ordinary Network and really describe some mode will be fixed some other not removable or thing Dynamic so I'm just curious how your approach can be applicable to search complex systems it's a great question so I think you'dhave to if you were going to do an online approach where you needed to sort of revise your behavior as the network was changing in real time um you would need to rely more heavily on graph theoretic uh approaches just because they're they're faster to compute and they're faster to update um I think you would probably be able to get some value from the node impact score itself but it would be difficult for you to expand that further like to look at the attack apparently because what does it mean to generate anaccurate Network that's changing over the course of the attack measurement and recommendation and simulation also three challenges having such kind of the dynamic models it's challenges okay hardest one to solve well the hardest one yeah the hardest one to solve from from my perspective would probably be simulation because now you're not just simulating what the attacker is doing you're simulating the environment as well and since both the attacker and the environment are changing continuously even in coding and representing thatinformation I think would be difficult much less doing an analysis on it if you had a good uh encoding or representation you might be able to do some analysis and pull something out of there but I think that that would probably be your biggest problem yeah yeah education across relationships in the real world perhaps great question uh obviously the real world graphs didn't come with trust relationships in there so uh we just decided the brand um and the same with the source and the goal nodes uh because the note thenetworks were not annotated in any way so we chose a source and a goal node at random in the network and then we injected a trust relationship uh between not between as we injected a trust relationship in the network as long as it didn't connect the source to the goal and then you permuted those yes and we we permuted those uh for so there were 50 um different permutations for each of the five data sets it seems low cost um so uh into those five examples right and then three of them will be worse together and two weresorry uh good morning so uh CDN is basically overlaying that and you do not expect that to be a very large scale right um and what I'm interested to understand how the application that is dominant on that network is going to determine the features that you are for the results that you are see I mean we want the future cooperation it great that's a great question because there are there are very interesting patterns that arise in the networks from the fact that they're cdns so for example in the CDN based networks there's oftenmultiple ways to get from one point to another because they don't want the global CDN to go down so they have redundant routes that allow you to Route uh traffic through a different location um and that really shows up in the results uh in that we can much more easily induce costs that way by routing the adversary through some circuitous route um with by VR mitigations um in ways that are not as easy with the research networks which are more focused on efficiency will have some private session with the committee members but the if you haveany questions I thought we were getting booted okay then uh yeah but


# file1

source: `{{ page.path }}`
